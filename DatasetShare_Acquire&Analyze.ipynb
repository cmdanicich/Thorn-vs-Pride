{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "import numpy as np\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "sws = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Share \n",
    "\n",
    "Dataset share starts here. I have run descriptive stats on the Thorns .txt file and have come to the following conclusions. \n",
    "\n",
    "- total tokens: 290,410\n",
    "- unique tokens: 81,364\n",
    "- average token length: 6.17\n",
    "- lexical diversity: .2802\n",
    "- top 10 tokens:\n",
    "    1. usa\n",
    "    2. soccer\n",
    "    3. sports\n",
    "    4. love\n",
    "    5. united\n",
    "    6. ca\n",
    "    7. new\n",
    "    8. fan\n",
    "    9. oregon\n",
    "    10. football"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Stats\n",
    "\n",
    "def get_patterns_1000(text)  :\n",
    "\n",
    "    text_clean = text.lower().split()# split on whitespace, shift to lowercase\n",
    "    \n",
    "    text_clean = [w for w in text_clean if w.isalpha() and w not in sw]\n",
    "    text_clean = [w for w in text_clean if w.isalpha() and w not in sws]\n",
    "    text_clean = [w for w in text_clean if w.isalpha() or '#' in w]\n",
    "\n",
    "            \n",
    "# Descriptives\n",
    "    total_tokens = len(text_clean)\n",
    "\n",
    "    unique_tokens = len(set(text_clean))\n",
    "\n",
    "    text_token_len = [len(w) for w in text_clean]\n",
    "    avg_token_len = np.mean(text_token_len)\n",
    "\n",
    "    lex_diversity = len(set(text_clean))/len(text_clean)\n",
    "\n",
    "    text_counter = Counter(text_clean)\n",
    "    top_10 = text_counter.most_common(10)\n",
    "    top_1000 = text_counter.most_common(1000)\n",
    "\n",
    "    # Fill in results dictionary\n",
    "    results = {'tokens':total_tokens,\n",
    "               'unique_tokens':unique_tokens,\n",
    "               'avg_token_length':avg_token_len,\n",
    "               'lexical_diversity':lex_diversity,\n",
    "               'top_10':top_10,\n",
    "               'top_1000':top_1000}\n",
    "        \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in text file - THORNS\n",
    "thorns = open(\"ThornsFC_followers.txt\",'r').read()\n",
    "\n",
    "thorns_results = get_patterns_1000(thorns) # call back to function using thorns text\n",
    "print(thorns_results) # results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO \n",
    "\n",
    "\n",
    "- ACQUIRE AND ANALYZE \n",
    "    - fix general word cloud \n",
    "    - make dataframe\n",
    "    - read location column \n",
    "    - make word cloud out of the locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire and Analyze\n",
    "\n",
    "Acquire and analyze starts here. Utilizing the dataset share Twitter accounts, I will build a word cloud that depicts the locations of where users are tweeting about from. First, I have to convert the .txt file to a .csv adnd then read it into a dataframe using pandas. I will then grab locations and turn them into a word cloud, as well as a general word cloud with all top tokens. It will be interesting to see if majority of users are tweeting from portland, OR or Oregon in general, as this is where the Thorns FC soccer team is located, or if users are coming from various places across the northwest region, or even if they are being tweeted about in rivals citys such as Orlando, Seattle, and Los Angeles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read txt file to csv so can create dataframe\n",
    "read_file = pd.read_csv(r'/Users/clairedanicich/Desktop/Text_Mining/Thorn-vs-Pride/')\n",
    "read_file.to_csv (r'/Users/clairedanicich/Desktop/Text_Mining/Thorn-vs-Pride/', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read new .csv file\n",
    "data = pd.read_csv(\"filename.csv\", skipinitialspace=True, usecols=fields)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to dataframe using Pandas\n",
    "df = pd.DataFrame(data=user_info, columns=['date', 'user', 'location', 'tweet'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check colums and read data in location column \n",
    "print(df.keys())\n",
    "print(df.colum_name_here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frequency for location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(40, 30))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to only plot unique words\n",
    "thorns = len(set(thorns_results))\n",
    "\n",
    "# Plot the word cloud\n",
    "thorns_string = str(thorns_results) #requires a string\n",
    "\n",
    "# Import package\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Generate word cloud\n",
    "#the .generate() is where you list your string\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='gray', colormap='bwr', \n",
    "                      collocations=False, stopwords = STOPWORDS).generate(thorns_string)\n",
    "\n",
    "# Plot\n",
    "plot_cloud(wordcloud)\n",
    "\n",
    "#### NEED TO REMOVE RESULTS WORDS (avg_token_length ETC) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thorns_results(user.location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
